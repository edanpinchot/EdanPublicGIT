# -*- coding: utf-8 -*-
"""BostonClassifiersCompared.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zglTY_wcgeW7Ao7TbjKsbJxOJHKPtDS9
"""

# Commented out IPython magic to ensure Python compatibility.
#import dependencies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
# %matplotlib inline

from sklearn.datasets import load_boston
boston = load_boston()

#transform the dataset into a dataframe
df_x = pd.DataFrame(boston.data, columns = boston.feature_names)
df_y = pd.DataFrame(boston.target)

#split data into 70% training and 30% testing data
x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size = 0.30, random_state = 0)

"""## Decision Tree:"""

from sklearn.tree import DecisionTreeRegressor
tree_model = DecisionTreeRegressor(max_depth=6)
dtree = tree_model.fit(x_train, y_train)

#print the decision tree's score / prediction quality
print(dtree.score(x_train, y_train))
dtreeScore = dtree.score(x_test, y_test)
print(dtree.score(x_test, y_test))

from sklearn.tree import plot_tree
plt.figure(figsize=(25,10))
a = plot_tree(regressor, 
              feature_names = df_x.columns, 
              class_names = df_y, 
              filled=True, 
              rounded=True, 
              fontsize=14)

"""## Voting Ensemble:"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import VotingRegressor

logistic = LinearRegression()
knn = KNeighborsRegressor()
dtree = DecisionTreeRegressor()

logistic.fit(x_train, y_train)
knn.fit(x_train, y_train)
dtree.fit(x_train, y_train)

votingEnsemble = VotingRegressor(
    estimators = [('lin', logistic), ('knn', knn), ('dt', dtree)])
ve = votingEnsemble.fit(x_train, y_train)

#print the voting ensemble's score / prediction quality
print(ve.score(x_train, y_train))
print(ve.score(x_test, y_test))

"""# Random Forest:"""

# Import Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor

# Create a Random Forest Regressor
reg = RandomForestRegressor()

# Train the model using the training sets 
rf = reg.fit(x_train, y_train)

#print the random forest's score / prediction quality
print(rf.score(x_train, y_train))
print(rf.score(x_test, y_test))

"""## Comparison:"""

#Compare the 3 classifiers used so far:
print("Decision Tree:    ", (dtreeScore * 100))
print("Voting Ensemble:  ", (ve.score(x_test, y_test) * 100))
print("Random Forest:    ", (rf.score(x_test, y_test) * 100))

"""## Gradient Boost / XGBoost"""

# Import XGBoost Regressor
from xgboost import XGBRegressor

#Create a XGBoost Regressor
reg = XGBRegressor()

# Train the model using the training sets 
xgb = reg.fit(x_train, y_train)

#print xgboost's "score" / prediction quality
print(xgb.score(x_train, y_train))
print(xgb.score(x_test, y_test))

# Import Gradient Boost Regressor
from sklearn.ensemble import GradientBoostingRegressor

#Create a Gradient Boost Regressor
reg = GradientBoostingRegressor()

# Train the model using the training sets 
gboost = reg.fit(x_train, y_train)

#print gradient boost's "score" / prediction quality
print(gboost.score(x_train, y_train))
print(gboost.score(x_test, y_test))

#Compare Gradient Boost vs. XGBoost:
print("XGBoost:         ", (xgb.score(x_test, y_test) * 100))
print("Gradient Boost:  ", (gboost.score(x_test, y_test) * 100))